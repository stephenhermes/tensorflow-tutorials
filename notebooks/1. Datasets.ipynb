{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "The preferred way to ingest data into TensorFlow estimators is by using the `tf.data.Dataset` class. There are a couple reasons for this:\n",
    "1. Datasets automatically manage memory and resources\n",
    "2. They separate data ingestion from modeling. This means that modeling steps can be ran concurrently with data i/o operations, speeding up training\n",
    "3. They make batching/randomization from giant datasets split up over multiple files easy.\n",
    "\n",
    "**Note:** There are lots of examples online using things like `QueueRunner`s (check what the thing actually is) that were popular before `Dataset`s were introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data\n",
    "\n",
    "We need some data to work with. To start, we will just use a few .csv files. Later on we'll talk about .tfrecords, which is the preferred data format for TensorFlow.\n",
    "\n",
    "Let's load up the Boston data set for the test data. To make things more interesting, we'll make the columns have some different data types and split it into several .csv files. (Clearly this entirely unnecessary to do with these data, but it will put us in a situation closer to reality.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "features, labels = boston.data, boston.target\n",
    "columns = [c.lower() for c in boston.feature_names]\n",
    "df = pd.DataFrame(features, columns=columns)\n",
    "df['chas'] = df['chas'].map({0.: 'Y', 1.: 'N'})\n",
    "df['rad'] = df['rad'].astype(np.int64)\n",
    "df['target'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus chas    nox     rm   age     dis  rad    tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31    Y  0.538  6.575  65.2  4.0900    1  296.0     15.3   \n",
       "1  0.02731   0.0   7.07    Y  0.469  6.421  78.9  4.9671    2  242.0     17.8   \n",
       "2  0.02729   0.0   7.07    Y  0.469  7.185  61.1  4.9671    2  242.0     17.8   \n",
       "3  0.03237   0.0   2.18    Y  0.458  6.998  45.8  6.0622    3  222.0     18.7   \n",
       "4  0.06905   0.0   2.18    Y  0.458  7.147  54.2  6.0622    3  222.0     18.7   \n",
       "\n",
       "        b  lstat  target  \n",
       "0  396.90   4.98    24.0  \n",
       "1  396.90   9.14    21.6  \n",
       "2  392.83   4.03    34.7  \n",
       "3  394.63   2.94    33.4  \n",
       "4  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into multiple files\n",
    "n_shards = 5\n",
    "shard_size = len(df) // n_shards\n",
    "\n",
    "data_dir = pathlib.Path('../data/sharded_data')\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir()\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "for i in range(n_shards):\n",
    "    idx_start = i * shard_size\n",
    "    idx_end = (i + 1) * shard_size\n",
    "    df.iloc[idx_start:idx_end].to_csv(data_dir / 'boston-{0}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading in data from a file\n",
    "\n",
    "The general way we get in data with a `Dataset` is by instantiating a Dataset object, converting it to an iterator using the `make_one_shot_iterator` method, and get a batch of data with the iterator's `get_next` method. The `get_next` method returns an op (*verify this...*) which is why it is called only once (instead of each time we want to get the next batch of data).\n",
    "\n",
    "Since we are reading in a single .csv, we use `TextLineDataset`, which reads in plain text files and returns a dataset where the rows of the text document are the records of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat,target'\n",
      "b'0.0837,45.0,3.44,Y,0.437,7.185,38.9,4.5667,5,398.0,15.2,396.9,5.39,34.9'\n",
      "b'0.05023,35.0,6.06,Y,0.4379,5.706,28.4,6.6407,1,304.0,16.9,394.02,12.43,17.1'\n"
     ]
    }
   ],
   "source": [
    "# Read a single file as text\n",
    "file = (data_dir / 'boston-0.csv').as_posix()\n",
    "dataset = tf.data.TextLineDataset(file)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    batch1 = sess.run(batch)\n",
    "    batch2 = sess.run(batch)\n",
    "    batch3 = sess.run(batch)\n",
    "    \n",
    "for b in (batch1, batch2, batch3):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the three batches we got are each just a single string, and we also got the text of the header. We don't want to be including the header at all in the data, and we want an array for each row, not just a single string. \n",
    "\n",
    "The way datasets are modified is by chaining on methods which change the behavior of the dataset. Dealing with the header is straight forward; we can just use the dataset's `skip` method to skip the first row. To parse the rows as arrays and not a single string, we use the `map` method, which will apply the same function to every row of the dataset. TensorFlow provides a `decode_csv` function which converts a string tensor representing a row of a csv file into a tuple of tensors for each field of the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0837, 45.0, 3.44, b'Y', 0.437, 7.185, 38.9, 4.5667, 5, 398.0, 15.2, 396.9, 5.39, 34.9)\n",
      "(0.05023, 35.0, 6.06, b'Y', 0.4379, 5.706, 28.4, 6.6407, 1, 304.0, 16.9, 394.02, 12.43, 17.1)\n",
      "(0.03961, 0.0, 5.19, b'Y', 0.515, 6.037, 34.5, 5.9853, 5, 224.0, 20.2, 396.9, 8.01, 21.1)\n"
     ]
    }
   ],
   "source": [
    "# Decode csv requires a list of default values to use for each tensor\n",
    "# produced. The defaults are passed as a list of lists.\n",
    "DEFAULT_VALUES = [[0.0]] * 14\n",
    "DEFAULT_VALUES[3] = ['_UNKNOWN']; DEFAULT_VALUES[8] = 0 \n",
    "\n",
    "def parse_row(row):\n",
    "    return tf.decode_csv(row, record_defaults=DEFAULT_VALUES)\n",
    "\n",
    "dataset = tf.data.TextLineDataset(file)\n",
    "dataset = dataset.skip(1) # skip the header\n",
    "dataset = dataset.map(parse_row) # convert string to array\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    batch1 = sess.run(batch)\n",
    "    batch2 = sess.run(batch)\n",
    "    batch3 = sess.run(batch)\n",
    "    \n",
    "for b in (batch1, batch2, batch3):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aside:* Since the `batch` op now produces a tuple of tensors instead of a single tensor, we're using `sess.run` instead of `batch.eval`. \n",
    "\n",
    "If all of our data is in a single file, that is it. We have our data input pipeline. We can apply additonal methods to spruce up our Dataset by shuffling the data, taking batches of more than just one element, improving memory management, and so on.\n",
    "\n",
    "\n",
    "## 3. Reading in data from multiple files\n",
    "\n",
    "Now that we've successfully read data from a single file, let's do multiple. The general idea is to first make a dataset of file names, and use the map method to make a dataset of datasets. This doesn't literally work: The iterator made from a dataset returns tensors, so has to have one of the allowable tensor datatypes. Hence, it can't return a dataset itself. However, there is a `flat_map` method which applies a function to the rows of all of the would-be datasets while simultaneously flattening them into a single dataset. This avoids ever actually having a dataset who returns tensors of type \"dataset\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data\\\\sharded_data\\\\boston-0.csv'\n",
      "b'data\\\\sharded_data\\\\boston-4.csv'\n",
      "b'data\\\\sharded_data\\\\boston-2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Can use wildcards for data with similar names\n",
    "file = (data_dir / 'boston-*.csv').as_posix()\n",
    "dataset = tf.data.Dataset.list_files(file)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    batch1 = sess.run(batch)\n",
    "    batch2 = sess.run(batch)\n",
    "    batch3 = sess.run(batch)\n",
    "\n",
    "# Just getting a dataset of individual file names\n",
    "for b in (batch1, batch2, batch3):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7258, 0.0, 8.14, b'Y', 0.538, 5.727, 69.5, 3.7965, 4, 307.0, 21.0, 390.95, 11.28, 18.2)\n",
      "(51.1358, 0.0, 18.1, b'Y', 0.597, 5.757, 100.0, 1.413, 24, 666.0, 20.2, 2.6, 10.11, 15.0)\n",
      "(0.05735, 0.0, 4.49, b'Y', 0.449, 6.63, 56.1, 4.4377, 3, 247.0, 18.5, 392.3, 6.53, 26.6)\n"
     ]
    }
   ],
   "source": [
    "# Convert each file name into a dataset and flat_map\n",
    "\n",
    "# Get dataset of file names\n",
    "dataset = tf.data.Dataset.list_files(file)\n",
    "# Combine all files into a single text dataset (without headers)\n",
    "dataset = dataset.flat_map(lambda f: tf.data.TextLineDataset(f).skip(1))\n",
    "# Convert each row into a tuple\n",
    "dataset = dataset.map(parse_row)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    batch1 = sess.run(batch)\n",
    "    batch2 = sess.run(batch)\n",
    "    batch3 = sess.run(batch)\n",
    "\n",
    "# Just getting a dataset of individual file names\n",
    "for b in (batch1, batch2, batch3):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Some handy methods\n",
    "\n",
    "While actually training a model, there are a few things we want to do:\n",
    "1. Shuffle the data\n",
    "2. Repeat the dataset for training over multiple epochs\n",
    "3. Get batches of data\n",
    "4. Preload the next batch of data while training...\n",
    "\n",
    "(We also want to feed data into the `Estimator` during training as a tuple consisting of a dict of features and a label. This can be done in the `parse_row` function we wrote above. We'll go into this in more detail when we talk about `Estimators`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2.44953, 3.67822], dtype=float32), array([0., 0.], dtype=float32), array([19.58, 18.1 ], dtype=float32), array([b'Y', b'Y'], dtype=object), array([0.605, 0.77 ], dtype=float32), array([6.402, 5.362], dtype=float32), array([95.2, 96.2], dtype=float32), array([2.2625, 2.1036], dtype=float32), array([ 5, 24]), array([403., 666.], dtype=float32), array([14.7, 20.2], dtype=float32), array([330.04, 380.79], dtype=float32), array([11.32, 10.19], dtype=float32), array([22.3, 20.8], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "n_epochs = 5\n",
    "batch_size = 2\n",
    "\n",
    "# Build data set\n",
    "dataset = tf.data.Dataset.list_files(file)\n",
    "dataset = dataset.flat_map(lambda f: tf.data.TextLineDataset(f).skip(1))\n",
    "dataset = dataset.map(parse_row)\n",
    "# Repeat the dataset\n",
    "dataset = dataset.repeat(n_epochs)\n",
    "# Shuffle data\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "# Get a batch of data\n",
    "dataset = dataset.batch(batch_size)\n",
    "# Preload next batch to speed up training\n",
    "dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "batch = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    batch1 = sess.run(batch)\n",
    "\n",
    "print(batch1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of remarks:\n",
    "1. The number of repeats can be set to `None` in which case (according to the TensorFlow documentation) the model being fed by the dataset will train indefinitely. I am not sure how long indefinitely actually is?\n",
    "2. When shuffling, the `buffer_size` parameter specifies how many records to read into memory and then shuffle. The smaller this number is, the less randomized the data will actually be; the larger it is the more memory is used. Here I am only reading in a KB of data into memory at a time. In real life you'd want to use several MB or a few GB if you got the ram for it. *I should check if buffer_size refers to the number of records or the max memory footprint of the loaded data...*\n",
    "3. For prefetching, `buffer_size` specifies how many records to load into memory in advance. This is useful for speeding up training by allowing the dataset to load and process the next batch of training data while the previous batch is being consumed by the model.\n",
    "\n",
    "There are a lot of other things that can be done to improve the efficiency of this bad boy, such as using \"fused ops\" which do several of these steps at once. For more information check out https://www.tensorflow.org/guide/performance/datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
